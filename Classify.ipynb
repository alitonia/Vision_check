{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial: Playing CartPole game with OpenAI-gym",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alitonia/Vision_check/blob/master/Classify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hok3LR8zjho",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to the AI STEM class\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this tutorial, we will build a intelligent agent from scratch to play CartPole game using Q-learning and Deep Neural Network\n",
        "-\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSEdIt_00NId",
        "colab_type": "text"
      },
      "source": [
        "# Setup environment\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Run the code below to automatical set up environment for the tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4042GWKJO69",
        "colab_type": "code",
        "outputId": "1b8d0f9f-e354-485a-e74a-ba4fc0bc343e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (42.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrNh2JZ03tD",
        "colab_type": "text"
      },
      "source": [
        "Import some packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6M-LUXeJYgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "import pdb\n",
        "import math\n",
        "from collections import deque\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czpuf9RG3I4c",
        "colab_type": "text"
      },
      "source": [
        "# Prepare a displaying window to visualize game\n",
        "Import a display package and configure a window for displaying CartPole game\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Xe9WScJa9m",
        "colab_type": "code",
        "outputId": "76766fe7-872d-4ee2-d02e-7c39acc83175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXxJ0f5B5dje",
        "colab_type": "text"
      },
      "source": [
        "Two functions:\n",
        "\n",
        "\n",
        "1.   show_video: to create a HTML video frame to display a game that is saved by OpenAI-gym\n",
        "2.   wrap_env: to wrap an existed environment in order to save an episode game.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjKgQD2CJfme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('/content/video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, '/content/video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmAxq6-uJyOg",
        "colab_type": "text"
      },
      "source": [
        "# Let's play CartPole game using random action\n",
        "\n",
        "---\n",
        "Create an OpenAI-gym environment for agent to interact. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LScsIRIWO8TB",
        "colab_type": "code",
        "outputId": "ea2eaf67-e7e2-4d7c-d2c3-5845372d9812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "test = env.reset()\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.02549702, -0.04831545,  0.00149151, -0.01454881])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iRoscdWv8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### addition function ####\n",
        "def step(action, reset=False):\n",
        "  if reset:\n",
        "    obs = env.reset()\n",
        "  else:\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "  image = env.render('rgb_array')\n",
        "  return obs, reward, done, image\n",
        "\n",
        "def get_new_state(state, predicting_angle):\n",
        "  state[2] = predicting_angle\n",
        "  return state\n",
        "\n",
        "# get_state(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iCJEPK4anr3",
        "colab_type": "code",
        "outputId": "a791c0ed-f1a8-4155-b9bf-eeae723f7032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_new_state(test, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.02642203,  0.02803793, 10.        ,  0.01621101])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my1ykhEgPAOn",
        "colab_type": "text"
      },
      "source": [
        "Playing a CartPole game with arbitrary action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxCQOn3lKWYp",
        "colab_type": "code",
        "outputId": "5ed014a3-5881-4686-d4c0-112ec59bd0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "state = env.reset()\n",
        "total_reward = 0\n",
        "while True:\n",
        "  #randomly sample an action from action space.\n",
        "  action = env.action_space.sample()\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  total_reward+=reward\n",
        "  if done:\n",
        "    break\n",
        "env.close()\n",
        "print(\"total_reward: \",total_reward)\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total_reward:  28.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVNMu5PuRAUx",
        "colab_type": "text"
      },
      "source": [
        "# Lets play CartPole using Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihjJLY-TTjfu",
        "colab_type": "text"
      },
      "source": [
        "Initialize some hyper-parameters to implement Q-learning algorithm "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wux98jymROn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "buckets = (1, 1, 6, 12,) # down-scaling feature space to discrete range\n",
        "n_episodes = 1000 # training episodes \n",
        "n_win_ticks = 195 # average ticks over 100 episodes required for win\n",
        "min_alpha = 0.1 # learning rate\n",
        "min_epsilon = 0.1 # exploration rate\n",
        "gamma = 1.0 # discount factor\n",
        "ada_divisor = 25 # only for development purposes\n",
        "quiet = False\n",
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "\n",
        "Q = np.zeros(buckets + (env.action_space.n,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-9ZmDW7T668",
        "colab_type": "text"
      },
      "source": [
        "Discretize continous observation into discrete state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AexCnNWEUIUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discretize(obs):\n",
        "        upper_bounds = [env.observation_space.high[0], 0.5, env.observation_space.high[2], math.radians(50)]\n",
        "        lower_bounds = [env.observation_space.low[0], -0.5, env.observation_space.low[2], -math.radians(50)]\n",
        "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
        "        new_obs = [int(round((buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
        "        new_obs = [min(buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
        "        return tuple(new_obs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agVdwh0B4Kck",
        "colab_type": "text"
      },
      "source": [
        "Some fundamental function to train an agent by utilizing Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOmccZcbUYZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_action(state, epsilon):\n",
        "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(Q[state])\n",
        "\n",
        "def update_q(state_old, action, reward, state_new, alpha):\n",
        "    Q[state_old][action] += alpha * (reward + gamma * np.max(Q[state_new]) - Q[state_old][action])\n",
        "\n",
        "def get_epsilon(t):\n",
        "    return max(min_epsilon, min(1, 1.0 - math.log10((t + 1) / ada_divisor)))\n",
        "\n",
        "def get_alpha(t):\n",
        "    return max(min_alpha, min(1.0, 1.0 - math.log10((t + 1) / ada_divisor)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO0O4BDpUhEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training():\n",
        "        scores = deque(maxlen=100)\n",
        "\n",
        "        for e in range(n_episodes):\n",
        "            current_state = discretize(env.reset())\n",
        "            alpha = get_alpha(e)\n",
        "            epsilon = get_epsilon(e)\n",
        "            done = False\n",
        "            i = 0\n",
        "\n",
        "            while not done:\n",
        "                action = choose_action(current_state, epsilon)\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "                new_state = discretize(obs)\n",
        "                update_q(current_state, action, reward, new_state, alpha)\n",
        "                current_state = new_state\n",
        "                i += 1\n",
        "\n",
        "            scores.append(i)\n",
        "            mean_score = np.mean(scores)\n",
        "            if mean_score >= n_win_ticks and e >= 100:\n",
        "                if not quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
        "                return e - 100\n",
        "            if e % 100 == 0 and not quiet:\n",
        "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
        "                print(scores)\n",
        "\n",
        "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
        "        return e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6nvjuUEUq4u",
        "colab_type": "code",
        "outputId": "0c949cb6-ea92-4b50-95f7-f7572a92ed68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "training()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Episode 0] - Mean survival time over last 100 episodes was 22.0 ticks.\n",
            "deque([22], maxlen=100)\n",
            "[Episode 100] - Mean survival time over last 100 episodes was 30.12 ticks.\n",
            "deque([12, 16, 14, 15, 13, 28, 13, 19, 22, 19, 15, 21, 20, 36, 13, 15, 16, 19, 12, 16, 18, 13, 12, 38, 12, 12, 15, 39, 12, 11, 21, 16, 11, 38, 23, 21, 36, 20, 26, 17, 17, 21, 12, 17, 12, 9, 13, 14, 16, 10, 12, 11, 16, 33, 12, 22, 17, 14, 21, 41, 14, 17, 63, 65, 37, 12, 18, 13, 33, 38, 18, 34, 17, 25, 58, 108, 18, 37, 42, 34, 27, 47, 34, 32, 41, 43, 35, 69, 39, 200, 86, 9, 61, 85, 126, 79, 59, 65, 29, 40], maxlen=100)\n",
            "[Episode 200] - Mean survival time over last 100 episodes was 118.97 ticks.\n",
            "deque([14, 37, 17, 8, 37, 33, 19, 21, 15, 16, 19, 42, 43, 28, 23, 35, 25, 67, 12, 35, 183, 24, 16, 101, 27, 12, 23, 28, 180, 153, 54, 200, 133, 46, 29, 20, 18, 32, 46, 30, 27, 79, 119, 42, 132, 56, 32, 28, 32, 28, 28, 16, 61, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 116, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200], maxlen=100)\n",
            "Ran 251 episodes. Solved after 151 trials âœ”\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3Clg6UbWFh3",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Q-learning algorithm\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCt7zSTEWNQt",
        "colab_type": "code",
        "outputId": "6a981414-7c16-4f3f-da47-b2473f3dc122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "obs = env.reset()\n",
        "state = discretize(obs)\n",
        "total_reward = 0\n",
        "while True:\n",
        "  action = choose_action(state, 0.1)\n",
        "  obs, reward, done, _ = env.step(action)\n",
        "  state = discretize(obs)\n",
        "  total_reward+=reward\n",
        "  if done:\n",
        "    break\n",
        "env.close()\n",
        "show_video()\n",
        "print(\"total_reward: {}\".format(total_reward))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-32d37b20b856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscretize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'discretize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXBvMq1B7Hsr",
        "colab_type": "text"
      },
      "source": [
        "# Let's play CartPole game using Deep Neural Network\n",
        "\n",
        "---\n",
        "import some packages for CartPole game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYnLTKy-NG8j",
        "colab_type": "code",
        "outputId": "40788e0e-91e7-4808-e669-db9148dcffcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "EPISODES = 100 # number of training episodes."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L8drNCo7k9v",
        "colab_type": "text"
      },
      "source": [
        "## Program an intelligent agent\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Program a agent using Q-learning and deep neural network to manipulate a cart in order to maximize balance time. Some fundamental functions list as follow:\n",
        "\n",
        "\n",
        "*  _init_: to initalize an agent for playing a game.\n",
        "*  *_build_model*: to build a deep neural network model for approximate a Q-function\n",
        "*   *remember:* to storage playing experience of agent in a replay buffer which is used later for training. \n",
        "*   *act:* to make a decision based on Q-value for controlling cart either \"move_to_left\" or \"move_to_right\". \n",
        "*   *replay*: to train a deep neural network by using experience from a replay buffer in order to predict Q-value of a pair state-action.\n",
        "*   *load*: load a pre-trained model of neural network.\n",
        "*   *save*: save a trained model of neural network.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DbQMGroJ86Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        # Homework: change the number of dense layer\n",
        "        model = Sequential()\n",
        "        model.add(Dense(6, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(6, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  \n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn4ILkGzAxOx",
        "colab_type": "text"
      },
      "source": [
        "## Train an agent using Deep Q-learning\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Main function for tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmo755Z_J-bl",
        "colab_type": "code",
        "outputId": "edcf450b-7d66-4c32-9a60-4cbd9dc5ecc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # create a wrap environment\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(200):\n",
        "           \n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        if e % 10 == 0:\n",
        "            agent.save(\"/content/cartpole-dqn_episode_\"+str(e)+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "episode: 0/100, score: 18, e: 1.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "episode: 1/100, score: 15, e: 0.99\n",
            "episode: 2/100, score: 73, e: 0.69\n",
            "episode: 3/100, score: 9, e: 0.66\n",
            "episode: 4/100, score: 12, e: 0.62\n",
            "episode: 5/100, score: 13, e: 0.58\n",
            "episode: 6/100, score: 13, e: 0.54\n",
            "episode: 7/100, score: 9, e: 0.52\n",
            "episode: 8/100, score: 12, e: 0.49\n",
            "episode: 9/100, score: 24, e: 0.43\n",
            "episode: 10/100, score: 11, e: 0.41\n",
            "episode: 11/100, score: 11, e: 0.39\n",
            "episode: 12/100, score: 17, e: 0.36\n",
            "episode: 13/100, score: 11, e: 0.34\n",
            "episode: 14/100, score: 22, e: 0.3\n",
            "episode: 15/100, score: 12, e: 0.28\n",
            "episode: 16/100, score: 12, e: 0.27\n",
            "episode: 17/100, score: 13, e: 0.25\n",
            "episode: 18/100, score: 29, e: 0.22\n",
            "episode: 19/100, score: 15, e: 0.2\n",
            "episode: 20/100, score: 20, e: 0.18\n",
            "episode: 21/100, score: 20, e: 0.16\n",
            "episode: 22/100, score: 14, e: 0.15\n",
            "episode: 23/100, score: 13, e: 0.14\n",
            "episode: 24/100, score: 13, e: 0.13\n",
            "episode: 25/100, score: 11, e: 0.13\n",
            "episode: 26/100, score: 36, e: 0.11\n",
            "episode: 27/100, score: 31, e: 0.091\n",
            "episode: 28/100, score: 44, e: 0.073\n",
            "episode: 29/100, score: 42, e: 0.059\n",
            "episode: 30/100, score: 28, e: 0.051\n",
            "episode: 31/100, score: 39, e: 0.042\n",
            "episode: 32/100, score: 48, e: 0.033\n",
            "episode: 33/100, score: 51, e: 0.026\n",
            "episode: 34/100, score: 73, e: 0.018\n",
            "episode: 35/100, score: 64, e: 0.013\n",
            "episode: 36/100, score: 63, e: 0.01\n",
            "episode: 37/100, score: 60, e: 0.01\n",
            "episode: 38/100, score: 56, e: 0.01\n",
            "episode: 39/100, score: 104, e: 0.01\n",
            "episode: 40/100, score: 125, e: 0.01\n",
            "episode: 41/100, score: 110, e: 0.01\n",
            "episode: 42/100, score: 80, e: 0.01\n",
            "episode: 43/100, score: 116, e: 0.01\n",
            "episode: 44/100, score: 62, e: 0.01\n",
            "episode: 45/100, score: 101, e: 0.01\n",
            "episode: 46/100, score: 68, e: 0.01\n",
            "episode: 47/100, score: 63, e: 0.01\n",
            "episode: 48/100, score: 65, e: 0.01\n",
            "episode: 49/100, score: 74, e: 0.01\n",
            "episode: 50/100, score: 124, e: 0.01\n",
            "episode: 51/100, score: 123, e: 0.01\n",
            "episode: 52/100, score: 135, e: 0.01\n",
            "episode: 53/100, score: 139, e: 0.01\n",
            "episode: 54/100, score: 152, e: 0.01\n",
            "episode: 55/100, score: 170, e: 0.01\n",
            "episode: 56/100, score: 199, e: 0.01\n",
            "episode: 57/100, score: 88, e: 0.01\n",
            "episode: 58/100, score: 71, e: 0.01\n",
            "episode: 59/100, score: 199, e: 0.01\n",
            "episode: 60/100, score: 61, e: 0.01\n",
            "episode: 61/100, score: 81, e: 0.01\n",
            "episode: 62/100, score: 68, e: 0.01\n",
            "episode: 63/100, score: 87, e: 0.01\n",
            "episode: 64/100, score: 188, e: 0.01\n",
            "episode: 65/100, score: 146, e: 0.01\n",
            "episode: 66/100, score: 38, e: 0.01\n",
            "episode: 67/100, score: 34, e: 0.01\n",
            "episode: 68/100, score: 41, e: 0.01\n",
            "episode: 69/100, score: 68, e: 0.01\n",
            "episode: 70/100, score: 68, e: 0.01\n",
            "episode: 71/100, score: 166, e: 0.01\n",
            "episode: 72/100, score: 150, e: 0.01\n",
            "episode: 73/100, score: 14, e: 0.01\n",
            "episode: 74/100, score: 11, e: 0.01\n",
            "episode: 75/100, score: 10, e: 0.01\n",
            "episode: 76/100, score: 148, e: 0.01\n",
            "episode: 77/100, score: 8, e: 0.01\n",
            "episode: 78/100, score: 11, e: 0.01\n",
            "episode: 79/100, score: 12, e: 0.01\n",
            "episode: 80/100, score: 7, e: 0.01\n",
            "episode: 81/100, score: 8, e: 0.01\n",
            "episode: 82/100, score: 9, e: 0.01\n",
            "episode: 83/100, score: 11, e: 0.01\n",
            "episode: 84/100, score: 63, e: 0.01\n",
            "episode: 85/100, score: 10, e: 0.01\n",
            "episode: 86/100, score: 7, e: 0.01\n",
            "episode: 87/100, score: 33, e: 0.01\n",
            "episode: 88/100, score: 40, e: 0.01\n",
            "episode: 89/100, score: 22, e: 0.01\n",
            "episode: 90/100, score: 69, e: 0.01\n",
            "episode: 91/100, score: 30, e: 0.01\n",
            "episode: 92/100, score: 25, e: 0.01\n",
            "episode: 93/100, score: 22, e: 0.01\n",
            "episode: 94/100, score: 145, e: 0.01\n",
            "episode: 95/100, score: 32, e: 0.01\n",
            "episode: 96/100, score: 40, e: 0.01\n",
            "episode: 97/100, score: 11, e: 0.01\n",
            "episode: 98/100, score: 16, e: 0.01\n",
            "episode: 99/100, score: 15, e: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8rb2gK5BSBZ",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate our trained model in playing Cartpole game using Deep Neural Network\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzonTv6lREGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make('CartPole-v0'))\n",
        "\n",
        "state = env.reset()\n",
        "state = np.reshape(state, [1, state_size])\n",
        "total_reward =0\n",
        "#load a trained model to decide proper action\n",
        "agent.load(\"cartpole-dqn_episode_30.h5\")\n",
        "while True:\n",
        "  #sample action from deep neural network\n",
        "  action = agent.act(state)\n",
        "  \n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  next_state = np.reshape(next_state, [1, state_size])\n",
        "  state = next_state\n",
        "  total_reward+=reward\n",
        "  if done:\n",
        "    break\n",
        "print(\"total_reward: \",total_reward)\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWMD7wTS6wYG",
        "colab_type": "code",
        "outputId": "66e3a37f-817f-4a01-e552-008d56db635d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "ll"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root 4096 Nov 27 22:38 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bW-UCSsetFP",
        "colab_type": "text"
      },
      "source": [
        "# Well done!\n",
        "You have finished the first tutorial!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Please share your Colab Notebook to the email tienmanhptit1994@gmail.com to submit your solution."
      ]
    }
  ]
}